{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: High-dimensional Gradient Descent\n",
    "\n",
    "In this exercise, you will learn the following\n",
    "\n",
    "* how to define a probability density function \n",
    "* how to fit a Gaussian model using maximum likelihood estimation\n",
    "* how to define a loss function via log likelihood\n",
    "* how to produce scatter and line plots using Matplotlib \n",
    "* how to train a Gaussian model using high-dimensional gradient descent/ascent\n",
    "* how to use pytorch for automated gradient descent/ascent \n",
    "\n",
    "To solve the exercise, you need to fill in some code below. Suggested solutions will be posted on Canvas later.\n",
    "\n",
    "#### NOTE: From this lab onward, we will use a different python development environment for tutorials. To do so, first you will need to remove your previous installations. Once log in your desktop, type\n",
    "\n",
    "`rm -rf .local`\n",
    "\n",
    "Afterwards, type \n",
    "\n",
    "`module load neural-comp`\n",
    "\n",
    "Then type\n",
    "\n",
    "`jupyter-notebook`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define probability density function and visualise data\n",
    "\n",
    "First, you need to load the dataset from the last tutorial, which contains 100000 independent observations sampled from a normal distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.cs.bham.ac.uk/~duanj/log/Gaussian_Distrbution/Gaussian_100000.csv\"\n",
    "data = np.genfromtxt(url, delimiter=\",\", skip_header=0, usecols=(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know our `data` above was sampled from a normal distribution, if we plot its histogram we will be able to see such a distribution visually.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data, 100, density=True)\n",
    "plt.ylabel('No of times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On top of the `data` and its histogram, we want to fit a Gaussian-like curve. To do so, we will need to define the probability density function of a Gaussian distribution. Below please write such probability density function with `mean` and `std` being model parameters and `x` representing the observations from the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_density_func(x, mean, std):\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to fit a curve using `prob_density_func` you defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_regrid = np.linspace(np.min(data),np.max(data), data.shape[0])\n",
    "mean, std = 5, 6\n",
    "pdf = prob_density_func(data_regrid, mean, std)\n",
    "\n",
    "plt.hist(data, 100, density=True)\n",
    "plt.ylabel('No of times')\n",
    "plt.plot(data_regrid, pdf, 'k--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ops! You can see the curve is not fitting the data very well. Please write down in the following your code which gives the correct fitting. Hint: using the maximum likelihood estimation results from the last lab tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a loss function\n",
    "\n",
    "As shown above, you can derive the explicit solutions with respect to the model parameters `mean` and `std`. However, this is not always the case for most deep learning models. Another way to compute a curve that can fit the data very well is to use gradient descent/ascent. To do so, we need two functions that defines the loss function and its gradients. \n",
    "\n",
    "From the last tutorial, we know that for a normal distribution the loss function is essentially the log likelihood of a joint probability density function. Below please write down your code to define such loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(data, mean, std):      \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply gradient descent/ascent to iteratively optimise the model parameters `mean` and `std`, we need to calculate the derivatives (ie. gradients) with respect to the parameters. Below please write down your code to implement `gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(data, mean, std):\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to visualise the landscape of the loss function. The idea is to create two 2D meshgrids, which respectively represent discrete values of mean and standard deviation. These values are then passed to the `log_likelihood` to compute the loss on a 2D meshgrid. Please write down your code in the place indicated by #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1D_grid = np.arange(1.0, 4.1, 0.1)\n",
    "std_1D_grid = np.arange(3.0, 6.1, 0.1)\n",
    "\n",
    "mean_2D_grid, std_2D_grid = np.meshgrid(mean_1D_grid, std_1D_grid)\n",
    "\n",
    "row = mean_1D_grid.shape[0]\n",
    "col = std_1D_grid.shape[0]\n",
    "\n",
    "loss_2D_grid = np.zeros([row, col])\n",
    "\n",
    "# write down your code here. Hint: you should use nested loops\n",
    "\n",
    "print('done the loop')\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(10)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot_surface(mean_2D_grid, std_2D_grid, loss_2D_grid)\n",
    "\n",
    "ax.set_xlabel('mean values')\n",
    "ax.set_ylabel('standard deviation')\n",
    "ax.set_zlabel('log likelihood')\n",
    "ax.view_init(25, -50)\n",
    "\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-dimensional gradient descent/ascent\n",
    "\n",
    "In this final section, we are going to use the gradient descend/ascent method to find the model parameters `mean` and `std`. In gradient descent/ascent, we usually work with an update scheme for the parameters using a small learning rate $\\epsilon$. The update rule can be defined as follows\n",
    "\n",
    "$$ [mean, std] \\leftarrow [mean, std] - \\epsilon \\times  gradient$$\n",
    "\n",
    "Here we use a learning rate $\\epsilon = 0.00002$ and initial parameters $mean=4$ and $std=6$. A for-loop will iteratively update the value of parameters (see the body of the for-loop). After each update, the new value of parameters will be plotted, and over time, it shows clearly how the parameters is updated until converging to the global optimum. \n",
    "\n",
    "Complete the code below so that the parameters `mean` and `std` is updated via gradient descent/ascent. Write down your code in the place indicated by #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import time\n",
    "\n",
    "# set up the canvas\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax1 = plt.subplot(121)\n",
    "ax2 = plt.subplot(122, projection='3d')\n",
    "\n",
    "plt.ion()\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "\n",
    "# set learning rate\n",
    "epsilon = 0.00002\n",
    "\n",
    "# Initial mean and std\n",
    "mean_, std_ = 4, 6\n",
    "parameters = np.array([mean_, std_])\n",
    " \n",
    "    \n",
    "for i in range(1, 50):   \n",
    "    \n",
    "    # write down your code here.\n",
    "           \n",
    "    ax1.clear()\n",
    "    ax1.hist(data, 100, density=True)\n",
    "    ax1.set_ylabel('No of times')\n",
    "    data_regrid = np.linspace(np.min(data),np.max(data), data.shape[0])\n",
    "    pdf = prob_density_func(data_regrid, parameters[0], parameters[1])\n",
    "    ax1.plot(data_regrid, pdf, 'k--')\n",
    "    \n",
    "    ax2.clear()\n",
    "    ax2.scatter3D(parameters[0], parameters[1], log_likelihood(data, parameters[0], parameters[1]), s=500, color=\"r\")\n",
    "    ax2.view_init(25, -50)\n",
    "    ax2.plot_surface(mean_2D_grid, std_2D_grid, loss_2D_grid)\n",
    "    ax2.set_xlabel('mean values')\n",
    "    ax2.set_ylabel('standard deviation')\n",
    "    ax2.set_zlabel('log likelihood')\n",
    "        \n",
    "    fig.canvas.draw()\n",
    "    time.sleep(0.01)\n",
    "    \n",
    "    \n",
    "print('gradient ascent mean {}'.format(parameters[0]))\n",
    "print('gradient ascent std {}'.format(parameters[1]))\n",
    "    \n",
    "print('ground truth mean {}'.format(np.mean(data)))\n",
    "print('ground truth std {}'.format(np.std(data)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction of pytorch\n",
    "\n",
    "So far, we have walked you through a 1D gradient descent case (ie. linear regression) and a high-dimensional 2D gradient descent case (ie. Gaussian curve fitting). It is now the time to introduce you [`pytorch`](https://pytorch.org/features/), which allows implementations of modern, powerful deep learning neural networks in a very simple yet efficient way. \n",
    "\n",
    "The question naturally raises as to what pytorch is? The answers to this are summarised as follows \n",
    "\n",
    "`Pytorch` is a python-based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "* A replacement for NumPy to use the power of GPUs\n",
    "* A deep learning research platform that provides maximum flexibility and speed\n",
    "\n",
    "As a start point, we ask you to quickly go through the tutorial from [here](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py) about some basic usage of pytorch tensors.  \n",
    "\n",
    "In addition to the use of GPUs, another very important feature about `pytorch` is *Automatic Differentiation*, which allows to numerically evaluate the derivative of a function/model specified by a computer program without any manual gradient calculations. That is to say we do not need something like `def gradient(data, mean, std)` defined above anymore. For a more detailed introduction of *Automatic Differentiation* in pytorch, please read [here](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py).\n",
    "\n",
    "In what follows, we will leverage the power of *Automatic Differentiation*. We will cover how to use GPUs using pytorch in later tutorials. As pytorch has been installed for our module, we can directly use it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "import torch # pytorch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import necessary pytorch libraries, which are `torch`, `torch.nn` and `torch optim`. These packages are explained as follows:\n",
    "\n",
    "* `torch`: a general purpose array library similar to Numpy that can do computations on GPUs when the tensor type is cast to (torch.cuda.TensorFloat)\n",
    "\n",
    "* `torch.nn`: a neural net library with common layers and cost functions\n",
    "* `torch.optim`: an optimisation package with common optimisation algorithms like SGD, Adam, etc\n",
    "\n",
    "The following describes a straightforward framework that reflects the coding structure of almost all other deep learning methods. You should get yourself really familiar with the framework. Its main building blocks (their orders may change) includes:\n",
    "\n",
    "* define a model \n",
    "* choose a optimiser\n",
    "* define a loss function \n",
    "* load data \n",
    "* train model parameters iteratively to minimise the loss\n",
    "\n",
    "Let's walk through each of steps above individually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a model class\n",
    "class prob_density_func(nn.Module):\n",
    "    def __init__(self, mean=1, std=1):\n",
    "        super(prob_density_func, self).__init__()\n",
    "       \n",
    "        self.mean = torch.nn.Parameter(torch.Tensor([mean]))  \n",
    "        self.std = torch.nn.Parameter(torch.Tensor([std]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return \n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return self.mean.item(), self.std.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a model class where one can see three parts: `__init__(self, mean=1, std=1)`, `forward(self, x)` and `extra_repr(self)`, which have the following meaning\n",
    "\n",
    "`__init__(self, mean=1, std=1)`: initialise all model parameters. Here we initialise `mean=1` and `std=1`. Of note, these two parameters can be overwritten later if needed. \n",
    "\n",
    "`forward(self, x)`: defines the computational model. Here our model is a probability density function (pdf) of a normal distribution. Please write down your code in this function. Note that you should use functions from `torch` instead of `numpy`, such as `torch.sqrt()` and `torch.exp()`. Also note that you should use `self.mean` and `self.std` for the pdf.\n",
    "\n",
    "`extra_repr`: prints the mean value and std value if invoked. \n",
    "\n",
    "We also note that by using the `self` keyword we can access the attributes and methods within the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare the model\n",
    "mean_, std_ = 4, 6\n",
    "model = prob_density_func(mean_, std_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we define the model with a new mean value and a new std value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an optimiser\n",
    "epsilon = 0.00002\n",
    "optimizer = optim.SGD(model.parameters(), lr=epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we choose a optimisation algorithm (ie. Stochastic Gradient Descent) to minimise the model defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "def loss_fuc(pdf):\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we define the loss function which is negative log likelihood of the joint probability density function of the distribution. Please write down your code above to implement the loss. Note that you should use functions from `torch` instead of `numpy`, such as `torch.log()` and `torch.sum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load input data\n",
    "url = \"http://www.cs.bham.ac.uk/~duanj/log/Gaussian_Distrbution/Gaussian_100000.csv\"\n",
    "data = np.genfromtxt(url, delimiter=\",\", skip_header=0, usecols=(0))\n",
    "data = torch.from_numpy(data.astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we load the dataset and then convert it to `torch` format such that *Automatic Differentiation* applies. \n",
    "\n",
    "In the following, we then start to train the model such that as iteration proceeds the optimal model parameters are searched via SGD. At the beginning, we need to set the gradients to zero (hence `optimizer.zero_grad()`) before starting to do backpropagation because PyTorch accumulates the gradients on subsequent backward passes. The results after feeding data to `model()` will be passed to `loss_fuc()` to compute the loss. Afterwards, `loss.backward()` computes `dloss/dx` for every model parameter `x`. In our application `x` is a vector of `mean` and `std`. These are accumulated into `x.grad` for every parameter `x`. In pseudo-code it reads:\n",
    "\n",
    "`x.grad += dloss/dx`\n",
    "\n",
    "Finally, `optimizer.step()` updates the value of `x` using the gradient `x.grad`. For example, the SGD optimizer performs:\n",
    "\n",
    "`x += -lr * x.grad`\n",
    "\n",
    "which is the same as what you have been shown in the previous section of manual gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the optimal values of mean and std using Automatic Differentiation and backpropagation\n",
    "for i in range(1, 500):\n",
    "    print ('iteration proceeds {}'.format(i))\n",
    "   \n",
    "    optimizer.zero_grad() \n",
    "    \n",
    "    pdf = model(data)\n",
    "    loss_value = loss_fuc(pdf)\n",
    "    \n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "mean, std = model.extra_repr()\n",
    "print('gradient descent mean via torch {}'.format(mean))\n",
    "print('gradient descent std via torch {}'.format(std))\n",
    "\n",
    "data = np.genfromtxt(url, delimiter=\",\", skip_header=0, usecols=(0))\n",
    "print('ground truth mean via maximum likelihood {}'.format(np.mean(data)))\n",
    "print('ground truth std via maximum likelihood {}'.format(np.std(data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If coding correctly, you will be able to see the model parameters `mean` and `std` computed via pytorch above are very close to the ground truth values computed from their analytical solutions. However, here gradients were computed automatically thanks to *Automatic Differentiation*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
