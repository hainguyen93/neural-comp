{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab6_questions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A-Uf0pguMU1J"
      },
      "source": [
        "# Neural Computation (Autumn 2019)\n",
        "# Lab 6: Automatic Differentiation (Autograd)\n",
        "\n",
        "Last week, we gave you a very brief introduction to Pytorch. This week, we will dive more deeply into the Deep Learning framework. By the end of this tutorial, you will know:\n",
        "\n",
        "- How to create tensors in Pytorch via `torch.Tensor`\n",
        "- How backpropgation is performed in Pytorch (via `autograd`)\n",
        "- How to split the dataset into training and validation sets\n",
        "- How to build a model using `nn.Module` and `nn.Linear` and train that model\n",
        "- How to evaluate the trained model using the validation set\n",
        "\n",
        "# Pytorch Tensors\n",
        "\n",
        "The most basic building block of any Deep Learning library is tensors, which are matrix-like data structures very similar to Numpy's ndarrays with the advantage being that Tensors can also be stored/used on a GPUs to accelerate computing. A scalar has zero dimension, so is called 0D tensor. A vector (i.e., 1D tensor) has one dimension, while a matrix (2D tensor) has two dimension. Note that we introduced tensors in Lab 3 (Advanced Numpy, Tensors & Tensor Operations). If you do not remember what a tensor is, please revisiting Lab 3 again.\n",
        "\n",
        "In this section, we will show you how to create a tensor using Pytorch. First, we need to `import` the package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PChD9B0LRuMk",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1nX9IT1vRxAk"
      },
      "source": [
        "You can construct a 4x3 randomly initialised matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CJrqe6ouWVOp",
        "colab": {}
      },
      "source": [
        "x = torch.rand(4,3)\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zsm3IfN0W_qb"
      },
      "source": [
        "Construct a tensor directly from data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dIuHjz_dW38R",
        "colab": {}
      },
      "source": [
        "x = torch.tensor([3, 4.5])\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SWkStdiyXM1K"
      },
      "source": [
        "You can retrieve the size of the tensor using `size` as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ey10qXjUXecc",
        "colab": {}
      },
      "source": [
        "print(x.size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TGIIR6MSXrpy"
      },
      "source": [
        "There are many operations that can be performed on tensors. The addition operation can be done as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O8xk1-cPX4o6",
        "colab": {}
      },
      "source": [
        "x = torch.rand(3,4)\n",
        "y = torch.rand(3,4)\n",
        "print(x+y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MksZH3NjYAvo"
      },
      "source": [
        "Or you can use another syntax:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KmuseWzZW7wZ",
        "colab": {}
      },
      "source": [
        "print(torch.add(x, y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Bq9lIO8TYOCA"
      },
      "source": [
        "You can also provide an output tensor as argument as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V5qygvlQYTrB",
        "colab": {}
      },
      "source": [
        "result = torch.empty(3,4)\n",
        "torch.add(x, y, out=result)\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M6Av5bfUYlYq"
      },
      "source": [
        "You can also convert a Torch Tensor to a Numpy Array using `numpy()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IYbK-qDHYuhy",
        "colab": {}
      },
      "source": [
        "x = torch.ones(5)\n",
        "print(x)\n",
        "print(x.dtype)\n",
        "\n",
        "y = x.numpy()\n",
        "print(y)\n",
        "print(y.dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kTTRw5aeZU_8"
      },
      "source": [
        "Convert Numpy Array to Torch Tensor using `torch.from_numpy()`. Notice how changing the np array has changed the Torch Tensor automatically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KlAB2g9eZcKi",
        "colab": {}
      },
      "source": [
        "x = np.ones(3)\n",
        "y = torch.from_numpy(x)\n",
        "\n",
        "np.add(x, 1, out=x)\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gx2EYILCZ69-"
      },
      "source": [
        "Tensors can be moved onto any device using the `.to` method. You can check whether a GPU is available using the `torch.cuda.is_available()` method. The `.to()` method sends your tensor to whatever device you specify, including your GPU (referred to as `cpu`) or your GPUs (referred to as `cuda` or `cuda:0`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H1SFLjDqaQFJ",
        "colab": {}
      },
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'  # check whether a GPU is available\n",
        "y = torch.ones(3,4, device=device)  # directly create a tensor on GPU\n",
        "x = torch.rand(3,4).to(device)      # or just use .to(device) \n",
        "z = x+y\n",
        "print(z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iRU_ta07bcll"
      },
      "source": [
        "As you have seen, `torch.Tensor` is the central class of the Pytorch package. If you now set its attribute `.require_grad` as `True`, it starts to track all operations on it. When you finish your computation you can call `.backward()` and have all the gradients computed automatically. The gradient for this tensor will be accumulated into `.grad` attribute.\n",
        "\n",
        "The following piece of codes creates a tensor and set `requires_grad=True` to track computation with it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XuorHYAKcATS",
        "colab": {}
      },
      "source": [
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ucWrE-uOcweu"
      },
      "source": [
        "Perform a tensor operation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5lxAKRQqc0Eq",
        "colab": {}
      },
      "source": [
        "y = x + 2\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kMxdIiPDc-OR"
      },
      "source": [
        "A `Function` is also an important class for autograd implementation in Pytorch. ``Tensor`` and ``Function`` are interconnected and build up an acyclic\n",
        "graph, that encodes a complete history of computation. Each tensor has\n",
        "a ``.grad_fn`` attribute that references a ``Function`` that has created\n",
        "the ``Tensor`` (except for Tensors created by the user - their\n",
        "`grad_fn is None`). In the example above, you see that `y` has a `grad_fn` since it was created as a result of an operation. We can print the value of that attribute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yhGbDBhadiLg",
        "colab": {}
      },
      "source": [
        "print(y.grad_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5QyluhmyeCpB"
      },
      "source": [
        "You can do more operations on `y`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XiBKtweUeBuh",
        "colab": {}
      },
      "source": [
        "z = y * y * 2\n",
        "out = z.mean()\n",
        "\n",
        "print(z)\n",
        "print(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e-9vAUHUeaPr"
      },
      "source": [
        "You can change an existing Tensor's `requires_grad` flag in-place by using `.requires_grad_(...)`. The input flag defaults to `False` if not given. Note that anu operation that mutates a tensor in-place is post-fixed with an `_`. For example, `x.copy_(y)` and `x.t_()` will change `x`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tUwR9Q5ge-hR",
        "colab": {}
      },
      "source": [
        "x = torch.randn(2, 2)\n",
        "print(x.requires_grad)  # default to False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZvLh7bvvfLQZ",
        "colab": {}
      },
      "source": [
        "x.requires_grad_(True)   # change the flag to True\n",
        "print(x.requires_grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Qf2ZMiDfYer",
        "colab": {}
      },
      "source": [
        "y = (x + 1).sum()\n",
        "print(y.requires_grad)\n",
        "print(y.grad_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zptrR5EcgJhK"
      },
      "source": [
        "# Gradients\n",
        "\n",
        "Let's backprop now. Consider the fowllowing piece of codes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CCPF0UHDgYlZ",
        "colab": {}
      },
      "source": [
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "y = x + 2\n",
        "z = y * y * 3\n",
        "out= z.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S9JCe0Y8gp2C",
        "colab": {}
      },
      "source": [
        "print(\"1: \", x)\n",
        "print(\"2: \", y)\n",
        "print(\"3: \", y.grad_fn)\n",
        "print(\"4: \", z)\n",
        "print(\"5: \", out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QlVpst3Pg_9N"
      },
      "source": [
        "If you want to compute the derivatives, you can call ``.backward()`` on\n",
        "a ``Tensor``. If ``Tensor`` is a scalar (i.e. it holds a one element\n",
        "data), you don’t need to specify any arguments to ``backward()``,\n",
        "however if it has more elements, you need to specify a ``gradient``\n",
        "argument that is a tensor of matching shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5HA3C9nohCuv",
        "colab": {}
      },
      "source": [
        "out.backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ibru7AK9hNtj"
      },
      "source": [
        "Since `out` contains a single scalar (i.e., mean), `out.backward()` is equivalent to `out.backward(torch.tensor(1.))`.  You can retrieve the gradient of d(out)/dx as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C1vBcQ62hS-B",
        "colab": {}
      },
      "source": [
        "print(x.grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dvVToP20Oq9D"
      },
      "source": [
        "You should have got a matrix of ``4.5``. Let’s call the ``out``\n",
        "*Tensor* “$o$”.\n",
        "We have that $o = \\frac{1}{4}\\sum_i z_i$,\n",
        "$z_i = 3(x_i+2)^2$ and $z_i\\bigr\\rvert_{x_i=1} = 27$.\n",
        "Therefore,\n",
        "$\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$, hence\n",
        "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MGDNbmVhihEh"
      },
      "source": [
        "Mathematically, if you have a vector valued function $\\vec{y}=f(\\vec{x})$,\n",
        "then the gradient of $\\vec{y}$ with respect to $\\vec{x}$\n",
        "is a Jacobian matrix:\n",
        "\n",
        "\\begin{align}J=\\left(\\begin{array}{ccc}\n",
        "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
        "   \\vdots & \\ddots & \\vdots\\\\\n",
        "   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        "   \\end{array}\\right)\\end{align}\n",
        "\n",
        "Generally speaking, ``torch.autograd`` is an engine for computing\n",
        "vector-Jacobian product. That is, given any vector\n",
        "$v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T}$,\n",
        "compute the product $v^{T}\\cdot J$. If $v$ happens to be\n",
        "the gradient of a scalar function $l=g\\left(\\vec{y}\\right)$,\n",
        "that is,\n",
        "$v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$,\n",
        "then by the chain rule, the vector-Jacobian product would be the\n",
        "gradient of $l$ with respect to $\\vec{x}$:\n",
        "\n",
        "\\begin{align}J^{T}\\cdot v=\\left(\\begin{array}{ccc}\n",
        "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
        "   \\vdots & \\ddots & \\vdots\\\\\n",
        "   \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        "   \\end{array}\\right)\\left(\\begin{array}{c}\n",
        "   \\frac{\\partial l}{\\partial y_{1}}\\\\\n",
        "   \\vdots\\\\\n",
        "   \\frac{\\partial l}{\\partial y_{m}}\n",
        "   \\end{array}\\right)=\\left(\\begin{array}{c}\n",
        "   \\frac{\\partial l}{\\partial x_{1}}\\\\\n",
        "   \\vdots\\\\\n",
        "   \\frac{\\partial l}{\\partial x_{n}}\n",
        "   \\end{array}\\right)\\end{align}\n",
        "\n",
        "(Note that $v^{T}\\cdot J$ gives a row vector which can be\n",
        "treated as a column vector by taking $J^{T}\\cdot v$.)\n",
        "\n",
        "This characteristic of vector-Jacobian product makes it very\n",
        "convenient to feed external gradients into a model that has\n",
        "non-scalar output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qhiFcpowpNhC"
      },
      "source": [
        "# How to define a model in Pytorch?\n",
        "\n",
        "## An example \n",
        "\n",
        "To illustrate this step, let's create some synthetic data. We choose a vector of some points for our feature `x` and create our labels (targets) using the following model $y=a + b \\cdot x + \\epsilon\\cdot  \\mathcal{N}$, where `a`, `b` and $\\epsilon$ are some constants and $\\mathcal{N}$ is noise following a standard normal distribution (mean=0, variance=1).\n",
        "\n",
        "Let's consider `a=1`, `b=2` and $\\epsilon=0.1$. Could you please generate a *column* vector of 100 points for our feature `x` and another *column* vector of the same size for our target `y`? Hint: you can generate samples which follow a standard normal distribution using `np.random.randn(...)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J7nAf4wLsGIy",
        "colab": {}
      },
      "source": [
        "# Add you codes here (use x and y as variable names for input and tagets)\n",
        "np.random.seed(42)\n",
        "x = ....\n",
        "y = ...."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Oycbp5Aksb4Z"
      },
      "source": [
        "Now we split the dataset into a trainning set (used for training, about 80% of the original dataset) and a validation set (used for testing, about 20%). We need to shuffle the array of indices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G4LnTtfsMU2F",
        "colab": {}
      },
      "source": [
        "indx = np.arange(100)  # indices for all data points in x\n",
        "print(\"Before shuffle: \\n\", indx)\n",
        "\n",
        "np.random.shuffle(indx)\n",
        "print(\"After shuffle: \\n\", indx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7xEEWsPn05kb"
      },
      "source": [
        "Now split the indices into two sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HBCl0__7tNAy",
        "colab": {}
      },
      "source": [
        "train_indx = ....  # first 80% for training\n",
        "val_indx = ....    # remaining 20% for validation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g-Sb75Sx1Jpm"
      },
      "source": [
        "Split the dataset into training and validation sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "prr746jgtZO7",
        "colab": {}
      },
      "source": [
        "# Generate inputs and targets for training step\n",
        "x_train, y_train = x[train_indx], y[train_indx]\n",
        "print(x_train.size, y_train.size)\n",
        "\n",
        "# Please generate inputs and targets for validation step\n",
        "x_val, y_val = ...."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "etPmmYt3-F56"
      },
      "source": [
        "We need to convert these Numpy's ndarray into Torch tensors of type `float`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k8HrWL-d-CIj",
        "colab": {}
      },
      "source": [
        "x_train_tensor = ....\n",
        "y_train_tensor = ....\n",
        "\n",
        "x_val_tensor = ....\n",
        "y_val_tensor = ...."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_YyKFUrH1UMT"
      },
      "source": [
        "We can also plot the points using the `scatter` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VJUfOAcKt9EL",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig, axs = plt.subplots(nrows = 1, ncols = 2)\n",
        "axs[0].scatter(x_train, y_train)  # plot the training dataset\n",
        "axs[1].scatter(x_val, y_val)      # plot the validation dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_i3x6addpIP7"
      },
      "source": [
        "## A simple linear gression model\n",
        "\n",
        "Let's difine a simple linear regression model to learn the values for two parameters `a` and `b` in the model above.\n",
        "\n",
        "As you might see last week, a model can be constructed in Pytorch using the `torch.nn` class. To explicitly define the model, you need to implement (at least) the following methods:\n",
        "\n",
        "- `__init__(self)`, which defines the components that make up the model. Here, you are not limited to defining parameters and other models (or layers in neural networks) as our model's attributes (see more about this later).\n",
        "- `forward(self, x)`, which performs the actual computation, that is, it ouputs a prediction, given the input `x`. You need not call the `forward(x)` method, and should call the whole model itself to perform a forward pass and output predictions.\n",
        "\n",
        "Out first model will look like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9EpKttBNzFhK",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class FirstModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # add your codes here (hint: last week's lab)\n",
        "        self.a = ....\n",
        "        self.b = ....\n",
        "\n",
        "    def forward(self, x):\n",
        "        return ...."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xGQPchZ6z_kT"
      },
      "source": [
        "In the `__init__` method, we define two parameters, `a` and `b`, using the `Parameter()` class. By doing this, you can invoke the `parameters()` method of our model to retrieve an iterator over all model's parameters, that we can feed our optimiser. Moreover, we can get the current values for all parameters using the model's `state_dict()` method.\n",
        "\n",
        "In the following, we will use stochastic gradient descent, i.e., the `SGD` method from `torch.optim` package, which takes two arguments: the list of model's parameters (`model.parameters()`) and the learning rate `lr`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QSFNtSSU7aeW",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Now we can create a model\n",
        "model = FirstModel().to(device)\n",
        "\n",
        "# we can also inspect its parameters\n",
        "print(\"Before training: \\n\", model.state_dict())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zQr1RUXhRizi",
        "colab": {}
      },
      "source": [
        "# set learning rate\n",
        "lr = 1e-1\n",
        "\n",
        "# set number of epoches, i.e., number of times we iterate through the training set\n",
        "epoches = 100\n",
        "\n",
        "# We use mean square error (MSELoss)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# PLEASE use stochastic gradient descent (SGD) to update a and b\n",
        "optimiser = ....\n",
        "\n",
        "for epoch in range(epoches):\n",
        "    model.train()             # set the model to training mode \n",
        "    optimiser.zero_grad()     # avoid accumulating gradients\n",
        "    y_pred = ....             # PLEASE calculate model's output\n",
        "    loss = ....               # PLEASE calculate the MSE loss\n",
        "    loss.backward()           # calculate gradients\n",
        "    optimiser.step()          # update model's params\n",
        "\n",
        "print(\"After training: \\n\", model.state_dict())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yL2y-zlk_P8v"
      },
      "source": [
        "When setting the number of epoches sufficiently large, you should be able to obtain some values for parameters `a` and `b` which are really close to their ground-truth values of 1 and 2. \n",
        "\n",
        "**Please change the number of epoches (`epoches`) and the learning rate (`lr`) to see how the derived values for the model's parameters have changed.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EdPqZj1qATwP"
      },
      "source": [
        "## Nested model\n",
        "\n",
        "In our model above, we manually created two parameters to perform a linear regression. We can also use Pytorch's `Linear` model as an attribute to our model, which results in a nested model. \n",
        "\n",
        "To do this, we need to change the `__int__` method, where we create an attribute that contains our nested `Linear` model. Also in the `forward()`, we will simply call the nested model itself to perform the forward pass.\n",
        "\n",
        "Our new model will look like:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "59rl5A8ABFZi",
        "colab": {}
      },
      "source": [
        "class NewModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # a simple linear layer with an input and an output\n",
        "        self.linear = nn.Linear(1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return ...."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MXR9UKr0B8OL"
      },
      "source": [
        "We can now call inspect the model's parameters by calling `parameters()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_IFAvr-nCCr9",
        "colab": {}
      },
      "source": [
        "new_model = NewModel().to(device)\n",
        "print(list(new_model.parameters()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H650FfseCjhj"
      },
      "source": [
        "You should see that the model has two parameters. The first one is the **weight** `b`, while the other one is the **bias** `a` in our linear model defined at the beginning (recall that $y=a + bx + \\epsilon \\cdot \\mathcal{N}$). You can see this clearly when using the `state_dict()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oDEgsLE3DB0z",
        "colab": {}
      },
      "source": [
        "print(new_model.state_dict())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2EvI7mDpIRbU",
        "colab": {}
      },
      "source": [
        "# print the model\n",
        "print(new_model)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hQJK86xVLfSV"
      },
      "source": [
        "Because we use SGD as our optimiser, so we need mini-batches (i.e., small subsets of our dataset) to feed the model. In Pytorch, we can use the `DataLoader` class to do this. We simply need to tell it which dataset (of `TensorDataset`) to use, the size of the mini-batch (or simply batch size). The loader is an iterator-like, which can loop over the dataset and fetch a adifferent mini-batch every time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pldG3XcJLjPF",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tKehfvlILlZQ"
      },
      "source": [
        "Let's see how we train the model now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tdXY9JdkGVXl",
        "colab": {}
      },
      "source": [
        "# PLEASE construct the model here\n",
        "new_model = ....\n",
        "\n",
        "print(\"Before training: \\n\", new_model.state_dict())\n",
        "lr = 1e-1\n",
        "epoches = 100\n",
        "\n",
        "# We use mean square error (MSELoss)\n",
        "loss_fn = ....\n",
        "\n",
        "# PLEASE use stochastic gradient descent (SGD) to update a and b\n",
        "optimiser = "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yWp83XLfSTD1",
        "colab": {}
      },
      "source": [
        "for epoch in range(epoches):\n",
        "    new_model.train() \n",
        "    for x_batch, y_batch in train_loader:\n",
        "        # send tensors to device (cpu/cuda)\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        # add your codes here\n",
        "        ....\n",
        "\n",
        "print(\"After training: \\n\", new_model.state_dict())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v2filI6iDWwk"
      },
      "source": [
        "So far, we have defined an optimiser, a loss function and a (nested) model. We realise that if we use the codes above, we have to modify it whenever we'd like to use a different optimiser or a different loss function. You might think of making our codes more generic. How about writing a function that takes three arguments (optimiser, loss function and a model) and perform the training step. You will see below how to write a function in Python which returns another function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fYkzbnblEBuc",
        "colab": {}
      },
      "source": [
        "def generic_code(model, loss_fn, optimiser):\n",
        "\n",
        "    # define a function inside another function\n",
        "    def train_step(x_batch, y_batch):\n",
        "        optimiser.zero_grad()\n",
        "        y_pred = ....           # forward pass\n",
        "        loss = ....             # calculate loss value\n",
        "        loss....                # autograd\n",
        "        optimiser....             # update parameters  \n",
        "        return loss.item()             # return the loss\n",
        "\n",
        "    # return the newly defined function\n",
        "    return train_step                # return a function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W1tW68QyFNNm"
      },
      "source": [
        "You can now perform the training process as usual:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-yQQxqmjFSlY",
        "colab": {}
      },
      "source": [
        "new_model = NewModel()\n",
        "print(new_model.state_dict())\n",
        "\n",
        "lr = 1e-1\n",
        "epoches = 100\n",
        "\n",
        "# We use mean square error (MSELoss)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# We also use stochastic gradient descent (SGD) to update a and b\n",
        "optimiser = optim.SGD(new_model.parameters(), lr=lr)\n",
        "\n",
        "train_step = generic_code(new_model, loss_fn, optimiser)\n",
        "# What is the type of train_step?\n",
        "\n",
        "# list to record the loss over training course\n",
        "losses = list()\n",
        "\n",
        "for epoch in range(epoches):\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        # calculate the loss and add it to the loss list\n",
        "        # add your codes here\n",
        "\n",
        "print(new_model.state_dict())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3vlTQRxXTczN"
      },
      "source": [
        "You can now plot the loss value over time as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P4DwOiVoH1u1",
        "colab": {}
      },
      "source": [
        "plt.plot(range(len(losses)), losses, label=\"Training loss\")\n",
        "plt.xlabel(\"Training iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w8VPPsolIfak"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "We have not used the validation set that we created at the beginning of the tutorial. We have split the dataset of 100 points into a training set (80%) and a validation set (20%). We now change our model to include the evaluation phase, that is, computing the validation loss. Intuitively speaking, since we do not use the validation set when training the model, so the validation set is actually *unseen* to our model, and turns out to be a good choice to inspect how well our model performs on unseen data (i.e., making predictions). \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V284foTANIn9"
      },
      "source": [
        "We first need to create a DataLoader (`batch_size=20`) for the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DnSltGM3KeV8",
        "colab": {}
      },
      "source": [
        "val_dataset = ....\n",
        "val_loader = ...."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Cgj7wSc5Nx10"
      },
      "source": [
        "Our training loop should look like this. Note that `with torch.no_grad()` helps you stop autograd from tracking history on Tensors with `requires_grad=True`. All you need to do is wrapping the code block in `with torch.no_grad():`. See the following piece of codes for more detail."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8QHRfaE_N-AT",
        "colab": {}
      },
      "source": [
        "new_model = NewModel().to(device)\n",
        "print(new_model.state_dict())\n",
        "\n",
        "lr = 1e-1\n",
        "epoches = 20\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "optimiser = optim.SGD(new_model.parameters(), lr=lr)\n",
        "\n",
        "losses = list()\n",
        "val_losses = list()\n",
        "train_step = generic_code(new_model, loss_fn, optimiser)\n",
        "\n",
        "for epoch in range(epoches):\n",
        "    new_model.train()\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        # Calculate the loss (using train_step function) \n",
        "        # and add it to the list called losses\n",
        "        ....\n",
        "\n",
        "        with torch.no_grad():       # fix the model's params\n",
        "            for x_val, y_val in val_loader:\n",
        "                new_model.eval()    # set model to evaluation mode\n",
        "                y_pred = ....       # model's output for an input of x_val\n",
        "                val_loss = ....     # validation loss\n",
        "                val_losses.append(....)\n",
        "\n",
        "print(new_model.state_dict())\n",
        "plt.plot(range(len(losses)), losses,'r', label=\"Training loss\")\n",
        "plt.plot(range(len(val_losses)), val_losses,'g', label=\"Validation loss\")\n",
        "plt.xlabel(\"Training iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "99zBFzBmxS8m"
      },
      "source": [
        "Given the learning rate in the range of `np.linspace(0.01, 0.1, 50)`. Could you plot a graph showing how the validation loss changes according to different values of the learning rate? Set the number of epoches to a sufficiently small value for a reasonable running time. For each value of the learning rate, you might calculate the **average validation loss** or the **minimum validation loss**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I-2dSuo3yO6l",
        "colab": {}
      },
      "source": [
        "# add your codes here\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}